{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# This notebook is for the project Portugal Overseas Identity - Historical Network Research\n",
    "\n",
    "@Agatha, @Michal: feel free to add anything, like explanations about the project, comments, etc.\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1- To improve accuracy on existing entity types. Entity types that already exist in the spaCy xx_ent_wiki_sm (international language, including Portuguese) model can be found here: https://spacy.io/docs/usage/entity-recognition#entity-types\n",
    "\n",
    "2- To extend the named entity recognizer. New entity types to be added to the model. Model will be saved as pt_poihnr. Entities to be added:\n",
    "- Role\n",
    "- Title?\n",
    "- Type of document?\n",
    "\n",
    "3- After having a satisfactory NER model, to get senders and recipients from data as well as their roles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import xx_ent_wiki_sm\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a new file with only the full_text column in it and without square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_fulltext_file():\n",
    "    df = pd.read_csv('portugal_documents.csv') \n",
    "    full_text = df['full_text'].values         \n",
    "\n",
    "    with open('poi_full_text.csv', 'w') as outcsv:\n",
    "        writer = csv.writer(outcsv)\n",
    "        writer.writerow(['full_text'])\n",
    "        chars_to_remove = ['[', ']']\n",
    "        rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "        for i in range(len(full_text)):\n",
    "            line = re.sub(rx, '', full_text[i])\n",
    "            writer.writerow([line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_fulltext_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV file and creating JSON/TXT files with the data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('poi_full_text.csv') #reads all the data and store in a dataframe\n",
    "full_text = df['full_text'].values    #gets only the 'full_tex' column data in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### DUMPING IN A JSON FILE #####\n",
    "def dump_json(full_text):\n",
    "    \n",
    "    #Identified mixed labels\n",
    "    #DATE as PER\n",
    "    months = ['Janeiro', 'Fevereiro', 'Março', 'Abril', 'Maio', 'Junho', 'Julho', \\\n",
    "              'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']    \n",
    "    \n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    nlp = xx_ent_wiki_sm.load() #loads NLP model for 'international' languages\n",
    "    train_poi = []\n",
    "    \n",
    "    roles = pd.read_csv('roles_poi.csv')['roles'].values  #reads the list of roles manually added to the csv file\n",
    "\n",
    "    random.seed(2001)\n",
    "    #for i in range(4230):  #sample of 2.5% of the whole corpus\n",
    "    for i in range(200):\n",
    "        a = random.randint(0,169221) #random sampling\n",
    "        doc = nlp(full_text[a].decode('utf-8'))\n",
    "        doc_dict = {'text':full_text[a].decode('utf-8'), 'entity':0}\n",
    "\n",
    "        date_match = re.search(r'\\d{4},(.*),\\ \\d{1,2}', full_text[a].decode('utf-8')) #search for date: YYYY, month, DD\n",
    "\n",
    "        #iterate through all the entities found\n",
    "        entity_list = []\n",
    "        for ent in doc.ents:\n",
    "\n",
    "            entity_dict = {'entity_text': ent.text.decode('utf-8'), 'start': ent.start_char, \\\n",
    "                           'end': ent.end_char, 'label': ent.label_.encode('utf-8')}\n",
    "\n",
    "            #correcting DATE entity for the training file\n",
    "            if ent.text.decode('utf-8') in months:\n",
    "                if date_match is not None:\n",
    "                    entity_dict = {'entity_text': date_match.group(0), 'start': date_match.span()[0], \\\n",
    "                           'end': date_match.span()[1], 'label': 'DATE'}\n",
    "                else:  \n",
    "                    entity_dict['label'] = 'DATE'\n",
    "\n",
    "            entity_list.append(entity_dict)\n",
    "            \n",
    "        ### Adds ROLE entities based on roles_poi.csv file to the entity list\n",
    "        for role in roles:\n",
    "            role_match = re.search(role.decode('utf-8'), full_text[a].decode('utf-8'))\n",
    "            if role_match is not None:\n",
    "                dict_role = {'entity_text': role_match.group(0), 'start': role_match.span()[0], \\\n",
    "                               'end': role_match.span()[1], 'label': 'ROLE'}\n",
    "                entity_list.append(dict_role)\n",
    "\n",
    "        ### Gets the list of sentences       \n",
    "        doc_dict['entity'] = entity_list                                 \n",
    "        train_poi.append(doc_dict)\n",
    "\n",
    "    ### In .json format    \n",
    "    output = {\n",
    "        'train_data': train_poi\n",
    "    }\n",
    "\n",
    "    ### Saves file\n",
    "    with open('train_poi_data.json', 'wb') as jsonfile:\n",
    "        json.dump(output,jsonfile,ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### WRITING THE JSON FILE #####\n",
    "dump_json(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DUMPING IN A TXT FILE #####\n",
    "def dump_txt(full_text):\n",
    "\n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    #Identified mixed labels\n",
    "    #DATE as PER\n",
    "    months = ['Janeiro', 'Fevereiro', 'Março', 'Abril', 'Maio', 'Junho', 'Julho', \\\n",
    "              'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']\n",
    "\n",
    "    nlp = xx_ent_wiki_sm.load()\n",
    "\n",
    "    roles = pd.read_csv('roles_poi.csv')['roles'].values  #reads the list of roles manually added to the csv file\n",
    "\n",
    "    g = open('train_poi_data.txt','w')\n",
    "\n",
    "    random.seed(2001)\n",
    "    for i in range(4230):  #sample of 2.5% of the whole corpus\n",
    "    #for i in range(200):\n",
    "        a = random.randint(0,169221) #random sampling\n",
    "        doc = nlp(full_text[a].decode('utf-8'))\n",
    "\n",
    "        date_match = re.search(r'\\d{4},(.*),\\ \\d{1,2}', full_text[a].decode('utf-8')) #search for date: YYYY, month, DD\n",
    "\n",
    "        g.write(doc.text.lstrip(' '))\n",
    "        g.write('\\n')\n",
    "\n",
    "        for ent in doc.ents:\n",
    "\n",
    "            #correcting DATE entity for the training file\n",
    "            if ent.text in months:\n",
    "                if date_match is not None:\n",
    "                    g.write('('+date_match.group(0)+'; '+str(date_match.span()[0])+'; '+str(date_match.span()[1])+'; DATE)')\n",
    "                    g.write('\\n')\n",
    "\n",
    "            else:   \n",
    "                g.write('('+ent.text+'; '+str(ent.start_char)+'; '+str(ent.end_char)+'; '+ent.label_.encode('utf-8')+')')\n",
    "                g.write('\\n')\n",
    "\n",
    "        g.write('\\n')\n",
    "        g.write('\\n')\n",
    "\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### WRITING THE TXT FILE #####\n",
    "dump_txt(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DUMPING IN A FOLDER INDIVIDUAL TXT FILEs #####\n",
    "def dump_txt_individual(full_text):\n",
    "    \n",
    "    reload(sys)  \n",
    "    sys.setdefaultencoding('utf8')\n",
    "\n",
    "    #Identified mixed labels\n",
    "    #DATE as PER\n",
    "    months = ['Janeiro', 'Fevereiro', 'Março', 'Abril', 'Maio', 'Junho', 'Julho', \\\n",
    "              'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']\n",
    "\n",
    "    nlp = xx_ent_wiki_sm.load()\n",
    "\n",
    "    roles = pd.read_csv('roles_poi.csv')['roles'].values  #reads the list of roles manually added to the csv file\n",
    "    \n",
    "    if not os.path.exists('train_poi_data'):\n",
    "        os.makedirs('train_poi_data')\n",
    "\n",
    "    random.seed(2001)\n",
    "    for i in range(4230):  #sample of 2.5% of the whole corpus\n",
    "    #for i in range(200):\n",
    "\n",
    "        a = random.randint(0,169221) #random sampling\n",
    "        doc = nlp(full_text[a].decode('utf-8'))\n",
    "\n",
    "        date_match = re.search(r'\\d{4},(.*),\\ \\d{1,2}', full_text[a].decode('utf-8')) #search for date: YYYY, month, DD\n",
    "        id_match = re.match(r'(\\d+)[-.]', full_text[a].decode('utf-8').lstrip(' '))\n",
    "\n",
    "        if id_match is not None:\n",
    "            g = open('train_poi_data/'+str(i)+'_'+id_match.group(1)+'.txt','w')\n",
    "        else:\n",
    "            g = open('train_poi_data/'+str(i)+'_noid.txt','w')\n",
    "\n",
    "        g.write(doc.text.lstrip(' '))\n",
    "        g.write('\\n')\n",
    "\n",
    "        for ent in doc.ents:\n",
    "\n",
    "            #correcting DATE entity for the training file\n",
    "            if ent.text in months:\n",
    "                if date_match is not None:\n",
    "                    g.write('('+date_match.group(0)+'; '+str(date_match.span()[0])+'; '+str(date_match.span()[1])+'; DATE)')\n",
    "                    g.write('\\n')\n",
    "\n",
    "            else:   \n",
    "                g.write('('+ent.text+'; '+str(ent.start_char)+'; '+str(ent.end_char)+'; '+ent.label_.encode('utf-8')+')')\n",
    "                g.write('\\n')\n",
    "\n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CREATING THE FOLDER AND WRITING INDIVIDUAL TXT FILES IN THERE #####\n",
    "dump_txt_individual(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Area!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
